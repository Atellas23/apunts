Exercise 1

/usr/bin/time and time report different time measures, though quite similar. The times reported by the application, when summed up agree with the elapsed time approximately.
If we now select the transpose ACCESS(i,j), the timings look less similar in usr and sys, but agree in real/elapsed. Again, when summing up the times reported by the application, they do somewhat agree with the elapsed/real time value.
----------

Exercise 2

In initmat.cpp, I added the line #include "rusage.h" at the beggining and the following lines at the beggining and the end of the main function:
```
int main(int argc, char *argv[])
{
   struct rusage r;
   int res_init = get_resource_usage(&r);
   ...
   int res_fini = get_resource_usage(&r);
   return 0;
}
```
The Makefile entry for initmat was subsequently modified to include rusage.cpp:
```
initmat: initmat.cpp
	g++ -fopt-info -fopenmp -g -O3 -o initmat initmat.cpp rusage.cpp
```
----------

Exercise 3

I added to the end of the main function, and after the int res_fini = ... line, the following lines of code:
```
   printf("User mode time: %f\n", timeval_to_ms(&r.ru_utime));
   printf("Maximum resident set size: %ld\n", r.ru_maxrss);
   printf("Number of voluntary context switches: %ld\n", r.ru_nvcsw);
   printf("Number of involuntary context switches: %ld\n", r.ru_nivcsw);
```
The results were the following:
- User mode time: 474.762 ms
- Maximum resident set size: 410528 kB
- Voluntary context switches: 0
- Involuntary context switches: 3
----------

Exercise 4

Using exercise4.sh, I gathered the following information:
- 1 thread:
User mode time: 477.482000
Maximum resident set size: 412596
Number of voluntary context switches: 0
Number of involuntary context switches: 9
- 2 threads:
User mode time: 607.547000
Maximum resident set size: 412720
Number of voluntary context switches: 2
Number of involuntary context switches: 13
- 4 threads:
User mode time: 728.616000
Maximum resident set size: 412656
Number of voluntary context switches: 6
Number of involuntary context switches: 71
- 8 threads:
User mode time: 715.657000
Maximum resident set size: 412812
Number of voluntary context switches: 19
Number of involuntary context switches: 201
----------

Exercise 5

For the proposed cases, we get the following information (using exercise5.sh):
2 sw threads, 1 hw thread
- Number of voluntary context switches: 3
- Number of involuntary context switches: 51
4 sw threads, 1 hw thread
- Number of voluntary context switches: 9
- Number of involuntary context switches: 73
4 sw threads, 2 hw threads
- Number of voluntary context switches: 9
- Number of involuntary context switches: 88
4 sw threads, 3 hw threads
- Number of voluntary context switches: 9
- Number of involuntary context switches: 35
In these conditions, we can see how voluntary context switches seem to be related to the number of software threads available, while involuntary context switches show more of a random pattern. This last thing makes sense, as the number of involuntary context switches might be related to the activity of the computer in the moment of execution.
----------

Exercise 6
When using WSL (Ubuntu 18.04 LTS on Windows 10 1903) the programs did compile but caused several segmentation faults when using exercise6.sh, and produced no output. I had everything planned but the execution did not succeed. As I had already written the following tables, I thought they would stay anyway in answers.txt:
----------------|---------------------------------|---------------------------------|
CPU USER TIME	|	  Indexing (i, j)	  |	    Indexing (j, i)	    |
----------------|----------------|----------------|----------------|----------------|
    Num CPUs	| Outer parallel | Inner parallel | Outer parallel | Inner parallel |
----------------|----------------|----------------|----------------|----------------|
	1	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	2	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	3	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	4	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|

----------------|---------------------------------|---------------------------------|
Vol. ctxt. sw.	|	  Indexing (i, j)	  |	    Indexing (j, i)	    |
----------------|----------------|----------------|----------------|----------------|
    Num CPUs	| Outer parallel | Inner parallel | Outer parallel | Inner parallel |
----------------|----------------|----------------|----------------|----------------|
	1	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	2	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	3	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|
	4	|		 |		  |		   |		    |
----------------|----------------|----------------|----------------|----------------|

----------

Exercise 7

First of all, tim gets the time of day according to the time that has passed since the Unix Epoch (1 Jan 1970) using gettimeofday and clock_gettime(CLOCK_REALTIME, ...). Then it looks for the resolution of each system clock (REALTIME, MONOTONIC, REALTIME_COARSE, MONOTONIC_COARSE, MONOTONIC_RAW, BOOTTIME, PROCESS_CPUTIME_ID, THREAD_CPUTIME_ID).
----------

Exercise 8

time_usleep1 computes the time it takes for the system to process HOWMANY usleep(1) syscalls. In the original case, HOWMANY=300000 and in my computer it takes between 5500 and 7300 ms.
Modifying the program, we can make it calculate the average time of all usleep(1) calls. Our experiment yields around 20.9 us. When we modify the parameter in usleep and make it sleep for 5 microseconds, it takes around 5.87 times more for the program to end, and also the mean time of usleep(5) is 127.76 us, around 6.11 times the mean time of usleep(1). For 10, we see a similar behavior: there is a non-linear scaling of the time the call takes with respect to the parameter.
----------

Exercise 9

It is clear from the results that getpid() is a far faster syscall than usleep(1): from time_getpid we get that it takes around 765.35 ms for the program to process 300000 getpid() calls, and that the average time it takes for one of these calls is around 1.633 us. So, quite faster than usleep(1).
----------

Exercise 10

We can now see that a full cycle of open+close is around 780us, far greater than getpid() and usleep(1).
----------

Exercise 11

This time, a full cycle of fork+exit+waitpid takes on average 4938.91 us. Slower than all of the previous ones. (measured using HOWMANY=30000)
----------

Exercise 12

When repeating this command five times, I got an average bandwidth of 1.34 GB/s (from 1.3, 1.3, 1.4, 1.3, 1.4).
dd if=/dev/zero of=/tmp/myfile.dat count=1024 bs=$((1024*1024))
Also, an average of 0.79 s to write the 1 GB to /tmp.
----------

Exercise 13

Repeating 5 times
dd if=/tmp/myfile.dat of=/dev/null
I got an average of 40.87 seconds and an average read bandwidth of 26.28 MB/s.
----------

Exercise 14

We can see that matrix.sse2 with 1 thread revolves between 1.4 and 3.2 GFlops. This same set of instructions, when running without thread restrictions, achieves between 3.4 and 9.8 GFlops. In the case of avx2, the numbers are much lower: in the single-thread case, it orbits around 2.2 GFlops. The case with all threads achieves a maximum of 12.3 GFlops and a minimum of 3.6 GFlops. So, we can see that sse2 achieves lower operation rates per second in general than avx2.