# 3. Optimal and Adaptive Filtering

## 3.1. Wiener-Hopf filter

### 3.1.1. Introduction

Several **estimation problems** can be modeled relying on a similar formulation:

*Given a set of data from an observed noisy process $x[n]$ and a desired target process $d[n]$ that we want to estimate, produce an estimation $y[n]$ of the target process by linear time-invariant filtering ($T[n]=h[n]$) of the observed samples.*

We assume **known stationary signal and noise spectra** (**correlation**), as well as **additive noise**. We will first assume Finite Impulse Response (FIR) filters, and afterwards we will delve into non-stationary scenarios.

#### Filter configuration

This formulation can be applied to a large family of problems that are commonly sorted into four wide classes:

- ##### System identification

We want to **identify a given system**, that can be real or some abstraction. We model this system as an LTI system plus an additive noise source $w[n]$.

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200429104811364.png" style="zoom:80%;" />
    </figure>
</center>

**Design and use:** we excite the system with a known signal $x[n]$ and obtain the filter that models the system.

The application assumes a noisy reference and noise-free observations.

- ##### System inversion

We want to **estimate a system and apply its inverse to the signal**. We model this system as an LTI system plus an additive noise source $w[n]$.

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200429105327334.png" style="zoom:80%;" />
    </figure>
</center>

**Design:** we excite the system with a known signal $x[n]$ and obtain the filter that models the system.<br>**Use:** the filter is concatenated to the system to recover the estimated signal.

The application assumes noisy observations and a noise-free reference.

- ##### Signal prediction

We estimate the value of a random signal at a given time instance $x[n_0]$, based on other time instance values ($x[n_0-1],x[n_0-2],\ldots$).

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200429112419068.png" style="zoom:80%;" />
    </figure>
</center>

**Design:** we compare the current signal value $x[n_0]$ with its estimation $y[n_0]$.<br>**Use:** The current signal value $x[n_0]$ may not be available and we produce an estimation. If $x[n_0]$ is available, we produce the prediction error $e[n_0]$.

The application assumes that observations and reference belong to the same noisy process.

- ##### Signal cancellation

We estimate the value of a primary signal which contains an interference. This interference has been isolated through other sensors in additional signals.

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613122920231.png" style="zoom:80%;" />
    </figure>
</center>

**Design:** we compare the primary signal $d[n]$ with the interference $x[n]$.<br>**Use:** we obtain the clean signal as the estimation error $e[n]$.

The application assumes that the noisy interferences are the observations, while the noisy signal together with the interferences are the reference.

### 3.1.2. Minimum Mean Square Error (MMSE) prediction

Given the generic formulation, we restrict the analysis to the **FIR filter** case. It is the **optimal solution** if $x[n]$ and $d[n]$ are Gaussian jointly distributed processes. Then, the filter is assumed to have a finite number of coefficients $N$. We use the **MSE** as an optimization criterion because it is mathematically treatable, leads to useful solutions and can be used as a benchmark for other solutions.

We will use the following notation:
$$
\newcommand{\u}{\underline}
\newcommand{\l}{\left}
\newcommand{\r}{\right}
\newcommand{\expected}[1]{\mathbb E\l[{#1}\r]}
h[n]*x[n]=\u h^T\u x[n],\quad\u x[n]=\begin{bmatrix}
x[n] \\ x[n-1] \\ \cdots \\ x[n-N+1]
\end{bmatrix}.
$$
So, say we have this generic signal case:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200505111414445.png" style="zoom:80%;" />
    </figure>
</center>

Then, the error is
$$
e[n]=d[n]-y[n]=d[n]-\underline h^T\underline x[n],
$$
and we want to minimize it:
$$
\min_\u{h}\expected{e^2[n]}=\min_\u{h}\expected{\l(d[n]-\u h^T\u x[n]\r)^2}.
$$

In order to do this, we will first prove what is called the **principle of orthogonality**: if the MSE is minimum, then it holds that
$$
\expected{e[n]\u x[n]}=\u0.
$$
If the MSE is minimum with respect to the filter, we know that the gradient is $\u0$. Then, if we develop this mathematically,
$$
\nabla_\u{h}\expected{e^2[n]}=
\expected{\nabla_\u{h}\l(d[n]-\u h^T\u x[n]\r)^2}=
\expected{-2\l(d[n]-\u h^T\u x[n]\r)\u x[n]}=
\u0\iff
-2\expected{e[n]\u x[n]}=\u0.
$$
So, we know that **the error is orthogonal to the observations**.

Now we want to develop some useful results of the MMSE prediction in a specific signal scenario:

- The observation process $x[n]$ can be split into two parts, $x[n]=a[n]+b[n]$.
- The reference process $d[n]$ can be split into two parts, $d[n]=a'[n]+c[n]$.

These parts of each process have the following correlation properties:

- $r_{ab}[l]=\expected{a[n+l]b[n]}=0$
- $r_{a'c}[l]=\expected{a'[n+l]c[n]}=0$
- $r_{aa'}[l]=\expected{a[n+l]a'[n]}\neq0$
- $r_{ac}[l]=\expected{a[n+l]c[n]}=0$
- $r_{a'b}[l]=\expected{a'[n+l]b[n]}=0$
- $r_{bc}[l]=\expected{b[n+l]c[n]}=0$

When using the filter that minimizes the MSE, $\u h_{opt}$, the following properties hold:

1. At any point in time, **the signal estimation and the error signal are not correlated**:

$$
\expected{e[n]y[n]}=\l[y[n]=\u h_{opt}^T\u x[n]\r]=\expected{e[n]\u h_{opt}^T\u x[n]}=\u h_{opt}^T\expected{e[n]\u x[n]}=\u0.
$$

2. The variance of the reference signal is greater or equal than the variance of the error signal:

$$
\begin{split}
\expected{d^2[n]}=[d[n]=y[n]+e[n]]=\expected{(y[n]+e[n])^2}=\\

\expected{d^2[n]}+2\expected{y[n]e[n]}+\expected{e^2[n]}=[\text{we are using the optimal filter}]=\\

\expected{d^2[n]}+\expected{e^2[n]}\geq\expected{e^2[n]}.
\end{split}
$$

3. If the observation and the reference signals are not correlated, the variance of the estimation is zero:

$$
\begin{split}
\expected{y^2[n]}=\expected{y[n](d[n]-e[n])}=\expected{y[n]d[n]}-\expected{y[n]e[n]}=\\
\expected{y[n]d[n]}=\expected{\u h_{opt}^T\u x[n]d[n]}=\u h_{opt}^T\expected{\u x[n]d[n]}=\\
\l[\expected{\u x[n]d[n]}=\u 0\r]=0.
\end{split}
$$

4. The minimum variance of the error signal is $\varepsilon=r_d[0]-\u h^T_{opt}\u r_{xd}$:

$$
\begin{split}
\varepsilon:=\expected{e^2[n]}\Big\vert_{\min}=\expected{e[n](d[n]-y[n])}=\expected{e[n]d[n]}-\expected{e[n]y[n]}=\\

\expected{e[n]d[n]}=\expected{\l(d[n]-\u h^T_{opt}\u x[n]\r)d[n]}=\expected{d^2[n]}-\expected{\u h^T_{opt}\u x[n]d[n]}=\\

r_d[0]-\u h^T_{opt}\expected{\u x[n]d[n]}=\\

r_d[0]-\u h^T_{opt}\u r_{xd}.
\end{split}
$$

We will analyze the previous properties for the signal setting we stated before:

(sic.)

### 3.1.3. The Wiener-Hopf filter

#### The Wiener-Hopf solution

So far, we have analyzed some properties of the optimal filter, but we are yet to obtain it:
$$
\l.\begin{matrix}
e[n] = d[n]-\u h^T\u x[n]\\
\expected{\u x[n]e[n]} = \u 0
\end{matrix}\r\}\implies\expected{\u x[n]\l(d[n]-\u h^T\u x[n]\r)}=\u 0.\\

\expected{\u x[n]\l(d[n]-\u h^T\u x[n]\r)}=\expected{\u x[n]d[n]}-\expected{\u x[n]\u h^T\u x[n]}=\u0\iff\\

\expected{\u x[n]d[n]}-\expected{\u x[n]\u x^T[n]\u h}=\u r_{xd}[0]-\expected{\u x[n]\u x^T[n]}\u h=\u0\iff\\

\u r_{xd}[0]-\u{\u R}_x[0]\u h=\u 0\iff\boxed{\u h_{opt}=\u{\u R}_x^{-1}\u r_{xd}.}
$$
So, this is the optimal filter in the sense of MSE minimization, with the matrix and the vector involved being
$$
\u r_{xd}=\expected{\u x[n]d[n]}=\begin{bmatrix}
\expected{x[n]d[n]} \\ \expected{x[n-1]d[n]} \\ \cdots \\ \expected{x[n-N+1]d[n]}
\end{bmatrix}=\begin{bmatrix}
r_{xd}[0] \\ r_{xd}[-1] \\ \cdots \\ r_{xd}[-N+1]
\end{bmatrix}:\text{the Cross-correlation vector}\\

\u{\u R}_x=\expected{\u x[n]\u x^T[n]}=\begin{bmatrix}
r_x[0] & r_x[1] & \cdots & r_x[N-1] \\
r_x[-1] & r_x[0] & \cdots & r_x[N-2] \\
\vdots & \vdots & \ddots & \vdots \\
r_x[-N+1] & r_x[-N+2] & \cdots & r_x[0]
\end{bmatrix}:\text{the Correlation matrix}
$$
The **optimal filter**, hence, depends on the **second order statistics** of the processes. We will analyze the properties of this correlation matrix, and also we will study what to do when such statistics are not available.

#### The error performance surface

The Wiener-Hopf filter is optimal at minimizing the MSE of the prediction; that is, the variance (power) of the error signal $e[n]$ (assuming it is zero-mean). We now want to study how does the MSE behave for an arbitrary filter. In order to do this, let us first develop a useful result: for any filter, the MSE can be expressed as

$$
\expected{e^2[n]}=\varepsilon+\l(\u h_{opt}-\u h\r)^T\u{\u R}_x\l(\u h_{opt}-\u h\r).
$$
So, let's see this result:
$$
\newcommand{\parenth}[1]{\left(#1\right)}
\newcommand{\bracketh}[1]{\left[#1\right]}
\begin{split}
\expected{e^2[n]}=\expected{\parenth{d[n]-\u h^T\u x[n]}\parenth{d[n]-\u h^T\u x[n]}}=\\

\expected{d^2[n]}-2\expected{\u h^T\u x[n]d[n]}+\expected{\u h^T\u x[n]\u h^T\u x[n]}=\\

r_d[0]-2\expected{\u h^T\u x[n]d[n]}+\expected{\u h^T\u x[n]\u x^T[n]\u h}=\\

r_d[0]-2\u h^T\expected{\u x[n]d[n]}+\u h^T\expected{\u x[n]\u x^T[n]}\u h=\\

r_d[0]-2\u h^T\u r_{xd}+\u h^T\u{\u R}_x\u h=\bracketh{\varepsilon=r_d[0]-\u h^T_{opt}\u r_{xd}}=\\

r_d[0]-\u h^T_{opt}\u r_{xd}+\u h^T_{opt}\u r_{xd}-2\u h^T\u r_{xd}+\u h^T\u{\u R}_x\u h=\\

\varepsilon+\u h^T_{opt}\u{\u R}_x\u h_{opt}-2\u h^T\u{\u R}_x\u h_{opt}+\u h^T\u{\u R}_x\u h=\\

\varepsilon+\u h^T_{opt}\u{\u R}_x\u h_{opt}-\u h^T\u{\u R}_x\u h_{opt}-\u h_{opt}^T\u{\u R}_x\u h+\u h^T\u{\u R}_x\u h\implies\\

\boxed{
\expected{e^2[n]}=\varepsilon+\parenth{\u h_{opt}-\u h}^T\u{\u R}_x\parenth{\u h_{opt}-\u h}.
}
\end{split}
$$
So, as we can see, the MSE of **any filter** is a quadratic function of the filter coefficients and always lies in an $N-$dimensional surface. As $\u{\u R}_x$ is positive definite, the quadratic function is **convex** and hence it has a unique extreme that is a **minimum**, which is exactly what we wanted. The reference signal $d[n]$ only impacts on the position and value of the optimal solution, and not on the shape of the surface. We can also note that, as $\u{\u R}_x$ is positive definite, **any deviation from the optimum filter increases the MSE**. This increase depends only on $\u{\u R}_x$, and so, only on $x[n]$. This fact will be very useful in the design of adaptive filters.

----

<span style='color:blue'>Example:</span> Imagine we have the following **signal cancellation** scenario. We estimate the value of a primary voice signal which contains an interference (voice+noise) in a helicopter, taken with a microphone. This interference has been isolated through other sensors in additional signals.

So, first of all we set our scenario:

- The microphone signal receives **voice**, **engine sound** and **sensor noise**.
- The reference sensor (noise isolation) receives **engine sound** and **noise**, but its engine sound is different than the one received through the microphone.

Our signals are:

- **Voice**: uncorrelated with the other signals.
- **Engine$_\text S$ (sensor) / Engine$_\text M$ (microphone)**: correlated signals with a helicopter-cabin effect.
- **Noise$_\text S$**: everything in the reference sensor that does not appear in the microphone. Uncorrelated.
- **Noise$_\text M$**: intrinsic system noise, low power, uncorrelated.

Then, mathematically we can represent all of this by
$$
\begin{split}
\text{MIC:}\ m[n]=v[n]+e_M[n]+\cancelto{}{w_M[n]}\\
\text{SENSOR:}\ s[n]=e_S[n]+w_S[n]
\end{split}
$$
Which filter configuration do we need? A **cancellation scenario** like the following:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613122920231.png" style="zoom:80%;" />
    </figure>
</center>

We want to obtain $e[n]$ as the **clean voice signal**. So, $x[n]=s[n]$ and $d[n]=m[n]$ will do the trick, as our filter will transform $x[n]=s[n]=e_S[n]+w_S[n]$ into a signal with an approximate value for the engine noise received in the microphone.

So, how do we solve the problem? It is already solved! We have three ways of solving it:

1. **Initial solution**: we depart from the fact that we want to minimize the MSE, so $\nabla_\u{h}\l(e^2[n]\r)=\u0$, and we solve the problem via the following equation:

$$
\nabla_\u h\l[\l((v[n]+e_M[n])-\u h^T(\u e_S[n]+\u w_S[n])\r)^2\r]=\u0.
$$

2. **Partial solution**: we depart from the principle of error-observation orthogonality, $\expected{\u x[n]e[n]}=0$, and we solve the problem via:

$$
\expected{\big[\u e_S[n]+\u w_S[n]\big]\big[(v[n]+e_M[n])-\u h^T(\u e_S[n]+\u w_S[n])\big]}=\u0.
$$

3. **Final result**: we directly use the (already proven) fact that the optimal filter is

$$
\u h_\text{opt}=\u{\u R}{}_x^{-1}\u r{}_{xd}.
$$

For simplicity purposes, we will use the third option. Then,
$$
\begin{split}
\u{\u R}{}_x=\expected{\u x[n]\u x^T[n]}=\big[x[n]=s[n]=e_S[n]+w_S[n]\big]=\\

\expected{\big(\u e_S[n]+\u w_S[n]\big)\big(\u e_S[n]+\u w_S[n]\big)^T}=\\

\expected{\u e_S[n]\u e_S^T[n]}+2\expected{\u e_S[n]\u w_S^T[n]}+\expected{\u w_S[n]\u w_S^T[n]}=\\

\big[\text{engine and noise uncorrelated}\big]=\\

\expected{\u e_S[n]\u e_S^T[n]}+\expected{\u w_S[n]\u w_S^T[n]}=\boxed{
\u{\u R}{}_{e_S}+\u{\u R}{}_{w_S}.
}
\end{split}
$$
For $\u r{}_{xd}$, we do the same:
$$
\begin{split}
\u r{}_{xd}=\expected{\u x[n]d[n]}=\big[\u x[n]=\u e_S[n]+\u w_S[n]\big]=\\
\big[d[n]=v[n]+e_M[n]\big]=\expected{\big[\u e_S[n]+\u w_S[n]\big]\big[v[n]+ e_M[n]\big]}=\\
\expected{\u e_S[n]\cdot v[n]+\u e_S[n]\cdot e_M[n]+\u w_S[n]\cdot v[n]+\u w_S[n]\cdot e_M[n]}=\\
\big[\text{voice and noise uncorrelated with the other signals}\big]=\\
\expected{\u e_S[n]\cdot e_M[n]}=\boxed{\u r{}_{e_Se_M}.}
\end{split}
$$
The final solution is, then,
$$
\boxed{\Big[\u{\u R}{}_{e_S}+\u{\u R}{}_{w_S}\Big]\u h_\text{opt}=\u r{}_{e_Se_M}.}
$$
The double function of the filter is evident in this solution: it **adapts the correlated part of $s[n]$** (with $m[n]$) while **cancelling the uncorrelated one**.

---

#### Wiener-Hopf filter using a finite number of samples

As we already know, the optimal filter depends on the second ordre statistics of the signals. However, in a typical case we only have a (small) **finite number of available samples** from both the observable and reference signals. In that case, we can **minimize an estimation of the mean square error** over the available set of samples.

Let us assume that we have $M$ samples of the reference signal and, given that the filter has $N$ coefficients, $M+N-1$ samples of the observation signal. We can define:
$$
\text{MSE}\equiv\frac{1}{M}\sum_{m=0}^{M-1}e^2[m]=\frac{1}{M}\sum_{m=0}^{M-1}\l(d[m]-\u h^T\u x[m]\r)^2.
$$
Let us write this expression as a combination of vectors. If we arrange the $M$ sample equations
$$
e[n]=d[n]-\u h^T\u x[n]
$$
in a vector, we can express the error as
$$
\u e^T=\u d^T-\u h^T\u{\u X},
$$
given that
$$
\u{\u X}=\begin{bmatrix}
x[0] & x[1] & \cdots & x[M-1]\\
x[-1] & x[0] & \cdots & x[M-2]\\
\vdots & \vdots & \ddots & \vdots\\
x[-N+1] & x[-N+2] & \cdots & x[M-N]
\end{bmatrix},\ \u d=\begin{bmatrix}d[0]\\d[1]\\\vdots\\d[M-1]\end{bmatrix},\ \u e=\begin{bmatrix}e[0]\\e[1]\\\vdots\\e[M-1]\end{bmatrix}
$$
The **optimal filter** should minimize the MSE:
$$
\text{MSE}=\frac{1}{M}\sum_{m=0}^{M-1}e^2[m]=\frac{1}{M}\u e^T\u e=\frac{1}{M}\l(\u d^T-\u h^T\u{\u X}\r)\l(\u d^T-\u h^T\u{\u X}\r)^T,\\
\nabla_\u h\text{MSE}=\u0\iff\nabla_\u h\frac{1}{M}\l(\u d^T-\u h^T\u{\u X}\r)\l(\u d-\u{\u X}^T\u h\r)=\u0\iff\\
\iff\nabla_\u h\frac{1}{M}\l(\u d^T\u d-\u d^T\u{\u X}^T\u h-\u h^T\u{\u X}\u d+\u h^T\u{\u X}\u{\u X}^T\u h\r)=\u0\iff\\
\iff\frac{1}{M}\l(-2\u{\u X}\u d+2\u{\u X}\u{\u X}^T\u h\r)=\u0\iff\boxed{\u h=\l(\u{\u X}\ \u{\u X}^T\r)^{-1}\u{\u X}\ \u d:=\u h_\text{opt}.}
$$
By comparison with the optimal expression having infinite samples, we can see that we are implicitly **estimating the cross-correlation vector and the correlation matrix**, based on available samples:
$$
\u h_\text{opt}=\u{\u R}{}_x^{-1}\u r{}_{xd},\qquad\u h_\text{opt}=\l(\u{\u X}\ \u{\u X}^T\r)^{-1}\u{\u X}\ \u d.
$$
The estimations would be
$$
\hat{\u r}{}_{xd}(\u x,\u d)=\frac{1}{M}\u{\u X}\ \u d=\frac{1}{M}\sum_{m=1}^M\u x[m]d[m],\\
\hat{\u{\u R}}{}_x(\u x)=\frac{1}{M}\u{\u X}\ \u{\u X}^T=\frac{1}{M}\sum_{m=1}^M\u x[m]\u x^T[m].
$$
We can interpret the optimal filter (MMSE) using a finite number of samples as an estimate of the Wineer-Hopf filter using exact second-order statistics:
$$
\boxed{\u h_\text{opt}=\u{\u R}{}_x^{-1}\u r{}_{xd}\implies\hat{\u h}_\text{opt}=\l(\u{\u X}\ \u{\u X}^T\r)^{-1}\u{\u X}\ \u d.}
$$
In order to assess this estimator, let us fix a (simple) **system identification scenario**:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613135531886.png" style="zoom:80%;" />
    </figure>
</center>

The application assumse noise-free observations (the known signal $\u{\u X}$) and noisy reference $(\u d^T=\u g^T\u{\u X}+\u w^T)$. The additive noise is modeled as white and Gaussian. Then, as we did in the previous chapter of the course,
$$
\u d^T=\u g^T\u{\u X}+\u w^T\implies\u w^T=\u d^T-\u g^T\u{\u X},\text{ with }\u{\u C}{}_w=\sigma^2\u{\u{\text{Id}}}.\\
f(\u x;\u g)=\frac{1}{\sqrt{(2\pi)^N\sigma^{2N}}}\cdot\exp{\l[-\frac{\l(\u d^T-\u g^T\u{\u X}\r)\l(\u d^T-\u g^T\u{\u X}\r)^T}{2\sigma^2}\r]}\implies\\
\implies\mathcal L(\u x;\u g)=\ln{f(\u x;\u g)}=-\frac{N}{2}\ln{2\pi\sigma^2}-\frac{1}{2\sigma^2}\l(\u d^T-\u g^T\u{\u X}\r)\l(\u d-\u{\u X}^T\u g\r)\implies\\
\implies\nabla_\u g\mathcal L(\u x;\u g)=-\frac{1}{2\sigma^2}\nabla_\u g\l(\u d^T\u d-\u d^T\u{\u X}^T\u g-\u g^T\u{\u X}\u d+\u g^T\u{\u X}\u{\u X}^T\u g\r)=\\
=\frac{1}{2\sigma^2}\l[2\u{\u X}\u d^T-2\u{\u X}\u{\u X}^T\u g\r]=\frac{\u{\u X}\u{\u X}^T}{\sigma^2}\l[\l(\u{\u X}\u{\u X}^T\r)^{-1}\u{\u X}\u d^T-\u g\r].
$$
Hence the efficient estimator is (left) and the Fisher information matrix is (right):
$$
\boxed{
\u g=\l(\u{\u X}\u{\u X}^T\r)^{-1}\u{\u X}\u d^T,\qquad\u{\u I}=\sigma^2\l(\u{\u X}\u{\u X}^T\r)^{-1}.
}
$$

As we can see, the estimator that we have developed for the Wiener-Hopf filter is a MVUE estimator (it's **efficient**).

## 3.2. Linear Prediction

### Introduction

In **signal prediction**, we estimate the value of a random signal at a given time instance ($x[n_0]$), based on other time instance values ($x[n_0-1],x[n_0-2],\ldots$).

**Design:** we compare the current signal value $x[n_0]$ with its estimation $y[n_0]$.<br>**Use:** the current signal value $x[n_0]$ may not be available and we produce an estimation. If $x[n_0]$ is available, we produce the estimation error $e[n_0]$.

The application assumes that observations and reference belong to the same noisy process. In the context of **linear prediction**, we can define three possible scenarios:

- **Forward prediction:** the current sample is estimated using only previous samples. For example, to forecast a given parameter value.
- **Backward prediction:** the current sample is estimated using only future samples. For example, for "remembering" a given value. Implies some delay.
- **Linear smoothing** (or interpolation)**:** the current sample is estimated combining past and future samples. For example, to recover a damaged signal.

<span style='color:gray'>**Note:** commonly in signal processing applications, what is important is the **ability to obtain a good estimation** of a sample, pretending that it is known, rather than forecasting it.</span>

### The Wiener-Hopf filter as a predictor

Let us analyze the FIR Wiener-Hopf filter in the context of **forward prediction**. Let us assume that we want to predict a given stationary process $s[n]$. In that case, $d[n]=s[n]$ and $\u x[n]=\u s[n-1]$. With this scenario, the Wiener-Hopf solution implies:
$$
\u r{}_{xd}=\expected{\u s[n-1]s[n]}=
\begin{bmatrix}\expected{s[n-1]s[n]}\\\expected{s[n-2]s[n]}\\\vdots\\\expected{s[n-N]s[n]}\end{bmatrix}=
\begin{bmatrix}r_s[-1]\\r_s[-2]\\\vdots\\r_s[-N]\end{bmatrix}=\boxed{\u r{}_s[-1]}\\
\u{\u R}{}_x=\expected{\u s[n-1]\u s^T[n-1]}=\begin{bmatrix}
r_s[0] & r_s[1] & \cdots & r_s[N-1]\\
r_s[-1] & r_s[0] & \cdots & r_s[N-2]\\
\vdots & \vdots & \ddots & \vdots\\
r_s[-N+1] & r_s[-N+2] & \cdots & r_s[0]\\
\end{bmatrix}=\boxed{\u{\u R}{}_{s}}
$$
So we have the following relations between the two schemes:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613164215057.png" style="zoom:90%;" />
    </figure>
</center>

$$
\begin{matrix}
d[n] & \implies & s[n]: & \text{reference signal}\implies\text{current sample}\\
\u x[n] & \implies & \u s[n-1]: & N\text{ data samples}\implies N\text{ previous samples}\\
\u h & \implies & \u h: & \text{filter (}N\text{ taps)}\implies\text{predictor filter (}N\text{ taps)}\\
y[n] & \implies & \hat s[n]: & \text{filtered signal}\implies\text{current predicted sample}\\
e[n] & \implies & e[n]: & \text{prediction error}\implies\text{prediction error}\\
\end{matrix}
$$

When the optimal filter is used:

- Error is orthogonal to data:

$$
\expected{\u s[n-1]e[n]}=\u0
$$

- The power of the error is lower than the power of the reference signal:

$$
\expected{s^2[n]}\geq\expected{e^2[n]}
$$

- The minimum error power is:

$$
\varepsilon=r_s[0]-\u h_\text{opt}^T\u r{}_s
$$

The expression for the optimal filter is
$$
\u{\u R}{}_s\u h_\text{opt}=\u r{}_s,
$$
and the power of the error for any filter $\u h$ is
$$
\expected{e^2[n]}=\varepsilon+\l(\u h-\u h_\text{opt}\r)^T\u{\u R}{}_s\l(\u h-\u h_\text{opt}\r)
$$

### Linear prediction for signal coding (LPC)

Assuming **stationarity**, the Wiener-Hopf filter minimizes the MSE between the process and its estimation (MMSE: **minimum error power**): signals are usually processed by (close to) stationary segments called **frames**. In speech coding, this is typically 20ms.

As the power of the error is lower than the power of the reference signal. That allows defining a **coding gain** $G_C$:
$$
\sigma^2_s=\expected{s^2[n]}\geq\expected{e^2[n]}=\sigma^2_e\implies\boxed{G_C=\frac{\sigma^2_s}{\sigma_e^2}.}
$$
Given a filter different from the optimal one (for example, the **quantized filter $\u h{}_q$**), the obtained error power and actual coding gain can be computed:
$$
\expected{e^2[n]}=\varepsilon+\l(\u h{}_q-\u h_\text{opt}\r)^T\u{\u R}{}_s\l(\u h{}_q-\u h_\text{opt}\r)
$$

#### Coder/Decoder structure

For each frame (assumed to be a stationary signal), the decoder receives the filter that has been used for predicting the signal and the prediction error. Assuming **amplitude-discrete signals** $s[n],\hat s[n],e[n]\in\mathbb Z$, the receiver can reconstruct the original signal $s[n]$ without loss.

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613165917524.png" style="zoom:80%;" />
    </figure>
</center>

**Internal variables** are kept when starting to process a new frame. We can see on the left the **coder system**, and on the right the **decoder system**. Let's see how the first few iterations of the codec structure work:

If $n=0$, the **first coding step** starts by observing that $\u s^T[n-1]=\u s^T[-1]=\u0$, so $\hat s[n]=\hat s[0]=\u h^T\u s[-1]=0$. Then, $e[n]=e[0]=s[0]-\hat s[0]=s[0]$, so the coder transmits the filter and this predicted value. Then, the decoder kicks in. The **first decoding step** starts by setting $e[n]=e[0]=s[0]$ and $\u s[n-1]=\u s[-1]=\u0$, so then $\hat s[n]=\hat s[0]=\u h^T\u s[-1]=0$ and then, it can reconstruct the $n-$th sample as $e[0]+\hat s[0]=s[0]$.

If $n=1$, $\u s^T[n-1]=\u s^T[0]=(s[0],0,\ldots,0)$. Then, the prediction for $s[1]$ is $\hat s[1]=\u h^T\u s[0]=h_1\cdot s[0]$. The prediction error is $e[1]=s[1]-\hat s[1]=s[1]-h_1s[0]$. The decoder receives this error and the filter, and it can easiliy reconstruct the signal, as $e[n]=e[1]=s[1]-h_1s[0]$ and it uses $\u s^T[n-1]=\u s^T[0]$. Then, its prediction is exactly the same for the lower part of the filter, $\hat s[n]=\hat s[1]=\u h^T\u s[0]=h_1s[0]$ and so, it recovers the value of the signal as $e[1]+\hat s[1]=s[1]-h_1s[0]+h_1s[0]=s[1]$.

#### Quantization of the prediction error

So far, when we talked about quantization we have concentrated on the **quantization of values** that come directly from a **signal** (voice, audio, image) or are **model coefficients** (filter coefficients). As we are interested in transmitting the prediction error, the following question is quite relevant: should the same strategy apply in the case of **quantizing prediction error samples**? The **coding scheme**, in that case, can be the following one:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613171730600.png" style="zoom:80%;" />
    </figure>
</center>

In this kind of situation, how does the decoder work? Well, it tries to recover a (quantized) version of the input signal following the next scheme:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613172930000.png" style="zoom:80%;" />
    </figure>
</center>
The decoder uses $\varepsilon_q[n]$ as the quantization error and $e_q[n]$ as the quantized error, so $e[n]=e_q[n]+\varepsilon_q[n]$. For simplicity, let us assume that the exact values of the $N$ coefficients of $\u h$ are available at the receiver side and so are the first $N$ samples of the signal $\u s[N-1]$. Then, the scheme evolves like this: at the $N$-th step,

- The coder sets $\u s^T[n-1]$ to be $\u s^T[N-1]=(s[N-1],\ldots,s[0])$. Then, its prediction is $\hat s[n]=\hat s[N]=\u h^T\u s[N-1]$. Then, the prediction error is $e[n]=e[N]=s[N]-\hat s[N]$, which is exactly $s[N]-\u h^T\u s[N-1]$. This prediction error is quantized and hence, $e_q[n]=e_q[N]=e[N]-\varepsilon_q[N]$.
- The decoder predicts $\tilde s[N]=\u h^T\u s_q[N-1]$. But, as we have already said, the first $N$ samples of the signal are available to the decoder. So, $\tilde s[N]=\u h^T\u s[N-1]=\hat s[N]$. Then, its reconstruction of the input signal is $s_q[N]=e_q[N]+\tilde s[N]=e[n]-\varepsilon_q[N]+\hat s[N]$. By definition, $e[N]=s[N]-\hat s[N]$, and so, $s_q[N]=s[N]-\hat s[N]-\varepsilon_q[N]+\hat s[N]=\boxed{s[N]-\varepsilon_q[N],}$ which is a kind of quantized version of the input signal.

At the $(N+1)-$th step,

- The coder sends $e_q[N+1]=e[N+1]-\varepsilon_q[N+1]$.
- The decoder predicts $\tilde s[N+1]=\u h^T\u s{}_q[N]=\u h^T\l(s[N]-\varepsilon_q[N],s[N-1],\ldots,s[1]\r)^T$, which after the filtering is equal to $\tilde s[N+1]=\u h^T\u s[N]-h_0\varepsilon_q[N]=\hat s[N+1]-h_0\varepsilon_q[N]$. So, its reconstruction of the input signal is $s_q[N+1]=e_q[N+1]+\tilde s[N+1]=e[N+1]-\varepsilon_q[N+1]+\hat s[N+1]-h_o\varepsilon_q[N]$. If we further develop this, we get that $s_q[N+1]=s[N+1]-\hat s[N+1]-\varepsilon_q[N+1]+\hat s[N+1]-h_0\varepsilon_q[N]$, and so $\hat s[N+1]$ cancels out and we get $s_q[N+1]=s[N+1]-\varepsilon_q[N+1]-h_0\varepsilon_q[N]$.

### Linear prediction coding of speech signals

- in speech signals high temporal correlation between close samples (can be appreciated in autocorrelation or spectral density)
- linear prediction -> higher prediction gains in voiced signals
- large increase in the performance comes from the fact of including nearby samples in the predictor (N=8-10) as well as samples that are near to one pitch period T apart. samples in the middle of this range do not improve the prediction gain
- **short term & long term prediction**
  - short term: information about the periodicity of the signal is not explained
  - to achieve better results usually a long term predictor is concatenated to the short term one

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613200649575.png" style="zoom:80%;" />
    </figure>
</center>

- short&long term predictor: intermediate samples not used
- long term: usually around 1-3 coefficients. can be used for pitch estimation (more on the slides 28-31 of the 3.2 section)

## 3.3. Adaptive Filtering

### Introduction

#### Need for adaptive filtering

In the four scenarios that were presented as examples of the Wiener-Hopf filter, we can distinguish two different classes:

- **System identification and inversion**: if the system (or the system model) that is to be processed varies in time, the W-H solution has to adapt to these variations.
- **System prediction and cancellation**: if the processes that are analyzed are non-stationary, the W-H solution has to track and adapt to their statistical variations.

#### Assessment of adaptive filtering

The goal of an adaptive filter is to **first find and then track** the optimum filter as quick and accurately as possible. There are different algorithms for implementing the filter adaptation. Therefore, we need **criteria for assessing the quality** of these algorithms:

- **Speed of convergence**: (or speed of adaptation) it measures the ability of the algorithm to bring the adaptive solution to the omptimal one, independently of the initial conditions. It is a transient-phase property.+
- **Misadjustment:** (or quality of adaptation) it measures the stability of the reached solution, once convergence is achieved. It is due to the randomness of the input data. It is a steady-phase property.
- **Tracking:** if the processes that are analyzed are non-stationary, the W-H solution has to track and adapt to their statistical variations. It is a steady-state property.
- **Complexity:** commonly, it is measured in terms of the number of operations that the algorithm requires to process a new sample, or time update. Additional concepts such as memory usage and parallelization properties can be analyzed.

### Steepest descent

Most adaptive filtering algorithms are obtained by simple **modifications or iterative methods** for solving deterministic optimization problems. In the sequel, we are going to study several aspects of **gradient-based optimization techniques**, from the theoretical point of view and still in a stationary scenario, as bases for the creation and understanding of adaptive methods.

#### Study of the error performance surface

The Wiener-hopf solution filter is optimal in the sense that **it minimizes the MSE of the prediction**; that is, the variance (power) of the error $e[n]$. Recall that, for any filter $\u h$, the MSE can be expressed as:
$$
\expected{e^2[n]}=\varepsilon+\l(\u h_\text{opt}-\u h\r)^T\u{\u R}{}_x\l(\u h_\text{opt}-\u h\r)
$$
This expression represents an hyper-surface in $\mathbb R^N$ with a minimum at $\u h_\text{opt}$.

---

<span style='color:blue'>**Example:**</span> Let us analyze the case for $N=2$; that is, $\u h^T=(h_0,h_1)$.
$$
\l.\begin{matrix}
\u h_\text{opt}-\u h=\Delta\u h=\begin{bmatrix}\Delta h_0\\\Delta h_1\end{bmatrix}\\
\u{\u R}{}_x=\big[x[n]\in\mathbb R\big]=\begin{bmatrix}r_x[0] & r_x[1]\\ r_x[1] & r_x[0]\end{bmatrix}\end{matrix}\r\}\implies\expected{e^2[n]}=\varepsilon+\Delta\u h^T\u{\u R}{}_x\Delta\u h
$$
Then, the MSE is a quadratic function that looks like this:
$$
\expected{e^2[n]}=\varepsilon+r_x[0](\Delta h_0)^2+2\Delta h_0\Delta h_1r_x[1]+r_x[0](\Delta h_1)^2=\\
\varepsilon+r_x[0]\Big(h_{0_\text{opt}}-h_0\Big)^2+2r_x[1]\Big(h_{0_\text{opt}}-h_0\Big)\Big(h_{1_\text{opt}}-h_1\Big)+r_x[0]\Big(h_{1_\text{opt}}-h_1\Big)^2
$$
This defines a paraboloid in 3-dimensional space. The **level curves** of this paraboloid are ellipses in the plane:

<center>
    <figure>
        <img src="3. Optimal and Adaptive Filtering.assets/image-20200613221433968.png" style="zoom:80%;" />
    </figure>
</center>

What is the optimum filter if:

- We have an observed signal $x[n]$ with **low correlation** between consecutive samples:

$$
\u{\u R}{}_x=\begin{bmatrix}1.1&0.1\\0.1&1.1\end{bmatrix},\ r_d[0]=0.9486,\ \u r{}_{xd}=\begin{bmatrix}0.5272\\-0.4458\end{bmatrix}
$$

Given this data we can compute:
$$
\u h_\text{opt}=\u{\u R}{}_x^{-1}\u r{}_{xd}\implies\u h_\text{opt}=\begin{bmatrix}0.5204\\-0.4526\end{bmatrix}\\
\varepsilon=r_d[0]-\u h_\text{opt}^T\u r{}_{xd}\implies\varepsilon=0.4725
$$

- We have an observed signal $x[n]$ with **high correlation** between consecutive samples:

$$
\u{\u R}{}_x=\begin{bmatrix}40&39\\39&40\end{bmatrix},\ r_d[0]=0.9486,\ \u r{}_{xd}=\begin{bmatrix}0.5272\\-0.4458\end{bmatrix}
$$

Given this data we can compute:
$$
\u h_\text{opt}=\u{\u R}{}_x^{-1}\u r{}_{xd}\implies\u h_\text{opt}=\begin{bmatrix}0.487\\-0.486\end{bmatrix}\\
\varepsilon=r_d[0]-\u h_\text{opt}^T\u r{}_{xd}\implies\varepsilon=0.5153
$$

----

#### The minimization algorithm

An iterative algorithm that obtains the minimum of the error performance surface should fulfill the following criteria: ($k$ is the iteration)
$$
\lim_{k\to\infty}\u h^k=\u h_\text{opt},\quad\lim_{k\to\infty}\expected{e^2[n]}=\varepsilon 
$$
The proposed recursion uses the information in the gradient of the function to be minimized:
$$
\u h^{k+1}=\u h^k-\frac12\mu\nabla_\u h\expected{e^2[n]}\Big\vert_{\u h^k}
$$
This is the **steepest descent** algorithm; it is based on the Taylor expansion around $\u h^k$. The positive parameter $\mu$ is known as the **step size**, and it determines the **speed of convergence** towards the optimum. The gradient $\nabla_\u h$ of the MSE is used at each iteration, evaluated at each $\u h^k$. At every point, it is perpendicular to the level curves, so **it does not always aim at the minimum**. Let's calculate this gradient in our setting, using $e[n]=d[n]-\u h^T\u x[n]$:
$$
\begin{split}
\nabla_\u h\expected{e^2[n]}=\nabla_\u h\expected{\l(d[n]-\u h^T\u x[n]\r)\l(d[n]-\u h^T\u x[n]\r)}=\\
\nabla_\u h\expected{d^2[n]-d[n]\u h^T\u x[n]-\u h^T\u x[n]d[n]+\u h^T\u x[n]\u h^T\u x[n]}=\\
\expected{-d[n]\u x[n]-\u x[n]d[n]+2\u x[n]\u x^T[n]\u h}=-2\u r{}_{xd}+2\u{\u R}{}_x\u h.
\end{split}
$$
If we restrict this expression to $\u h^k$ and substitute the result into the recursion, we get:
$$
\boxed{\u h^{k+1}=\u h^k+\mu\l(\u r{}_{xd}-\u{\u R}{}_x\u h^k\r).}
$$

#### Convergence analysis

Let us start analyzing the **one-dimension** case, $N=1\implies\u h=h_0$:
$$
\u{\u R}{}_x=r_x[0]\equiv\lambda\implies\expected{e^2[n]}=\varepsilon+\lambda\Big(h_{0_\text{opt}}-h_0\Big)^2,\\
h_0^{k+1}=h_0^k-\frac12\mu\frac{\part}{\part h_0}\expected{e^2[n]}\Big\vert_{h_0^k},\\
h_0^{k+1}=h_0^k-\frac12\mu\Big(2\lambda h_0^k-2\lambda h_{0_\text{opt}}\Big)=h_0^k+\mu\lambda\Big(h_{0_\text{opt}}-h_0^k\Big)=(1-\mu\lambda)h_0^k+\mu\lambda h_{0_\text{opt}}
$$
If we now subtract $h_{0_\text{opt}}$ from both sides, we get
$$
h_0^{k+1}-h_{0_\text{opt}}=(1-\mu\lambda)\Big(h_0^k-h_{0_\text{opt}}\Big).
$$
This expression allows for a change of variable that simplifies the analysis of convergence: if we call $z$ to the difference in both sides of the equation, we have
$$
z^{k+1}=(1-\mu\lambda)z^k\implies\boxed{z^{k+1}=(1-\mu\lambda)^kz^0.}
$$
And, as the $\u h^k$ converge to $\u h_\text{opt}$, the $z^k$ converge to 0. The **convergence range** is:
$$
\lvert1-\mu\lambda\rvert<1\iff\l\{\begin{matrix}1-\mu\lambda<1\\-1+\mu\lambda<1\end{matrix}\r\}\iff0<\mu\lambda<2\implies\boxed{0<\mu<\frac2\lambda.}
$$
The extreme cases' behaviour is pretty obvious: if $\mu=0$, then the $z^k$ don't move at all; if $\mu=\frac2\lambda$, then the $z^k$ change signs at each iteration because of the $(-1)^k$. In the intermediate cases, the following happens:

- $0<\mu<\frac1\lambda$: the $z^k$ tend towards 0 slowly, as $(1-\mu\lambda)$ is between 0 and 1.
- $\mu=\frac1\lambda$: $z^1=0$, as $(1-\mu\lambda)=0$.
- $\frac1\lambda<\mu<\frac2\lambda$: the $z^k$ hop from one side to the other of 0, while approaching it.

To extend the previous analysis to the $N-$dimensional case, we need to establish some properties of the **correlation matrix**:

- It is **semipositive definite**: for any vector $\u v$,

$$
\u v^T\u{\u R}{}_x\u v=\u v^T\expected{\u x\u x^T}\u v=\expected{\u v^T\u x\u x^T\u v}=\Big[\u v^T\u x=\alpha\Big]=\expected{\alpha^2}\geq0.
$$

- Because of the previous point, its **eigenvalues are non-negative**:

$$
\newcommand{\uu}[1]{\u{\u {#1}}}
\u{\u R}{}_x\u u=\lambda\u u\implies\u u^T\u{\u R}{}_x\u u=\u u^T\lambda\u u\implies\lambda=\frac{\u u^T\u{\u R}{}_x\u u}{\u u^T\lambda\u u}=\frac{\alpha^2}{\|\u u\|^2}\geq0.
$$

- Via the **spectral theorem**, we know that it can be decomposed into $\uu R{}_x=\uu U\uu\Lambda\uu U^T$, where $\uu\Lambda$ is a diagonal matrix and $\uu U$ is an orthogonal or unitary matrix.

We are going to perform a **change of variable** analogous to that in the 1-D case. Nevertheless, since we are in an $N-$dimensional problem, we have to account for a **displacement** and a **rotation**:
$$
\u h^{k+1}=\u h^k+\mu\Big(\u r{}_{xd}-\uu R{}_x\u h^k\Big)\\
\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\u h^k+\mu\u r{}_{xd}-\u h_\text{opt}\\
\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\u h^k+\mu\uu R{}_x\u h_\text{opt}-\u h_\text{opt}\\
\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\u h^k-\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\u h_\text{opt}\\
\boxed{\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\Big(\u h^k-\u h_\text{opt}\Big).}
$$
Now, we can compensate for the **displacement**. In order to obtain the **rotation**, we decompose $\uu R{}_x$ into its spectral decomposition, $\uu U\uu\Lambda\uu U^T$, and
$$
\newcommand{\id}{\uu{\text{Id}}}
\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu R{}_x\Big)\Big(\u h^k-\u h_\text{opt}\Big)\\
\u h^{k+1}-\u h_\text{opt}=\Big(\uu{\text{Id}}-\mu\uu U\uu\Lambda\uu U^T\Big)\Big(\u h^k-\u h_\text{opt}\Big)\\
\uu U^T\Big(\u h^{k+1}-\u h_\text{opt}\Big)=\uu U^T\Big(\id-\mu\uu U\uu\Lambda\uu U^T\Big)\Big(\u h^k-\u h_\text{opt}\Big)\\
\uu U^T\Big(\u h^{k+1}-\u h_\text{opt}\Big)=\Big(\uu U^T-\mu\uu U^T\uu U\uu\Lambda\uu U^T\Big)\Big(\u h^k-\u h_\text{opt}\Big)\\
\uu U^T\Big(\u h^{k+1}-\u h_\text{opt}\Big)=\Big(\uu U^T-\mu\uu\Lambda\uu U^T\Big)\Big(\u h^k-\u h_\text{opt}\Big)\\
\uu U^T\Big(\u h^{k+1}-\u h_\text{opt}\Big)=\Big(\id-\mu\uu\Lambda\Big)\uu U^T\Big(\u h^k-\u h_\text{opt}\Big)\\
\boxed{\uu U^T\Big(\u h^{k+1}-\u h_\text{opt}\Big)\equiv\u z^k\implies\u z^{k+1}=\Big(\id-\mu\uu\Lambda\Big)\u z^k.}
$$

With this result, the different dimensions of the optimization problem have been decoupled, so every component can be analyzed separately:
$$
z_i^{k+1}=(1-\mu\lambda_i)z_i^k\implies z_i^{k+1}=(1-\mu\lambda_i)^kz_i^0\implies\lim_{k\to\infty}z_i^k=\lim_{k\to\infty}(1-\mu\lambda_i)^kz_i^0
$$

We have already solved the 1-D problem, so we know that for each dimension we have $0<\mu<\frac{2}{\lambda_i}$. Since we want to use the same $\mu$ for all dimensions, the (theoretical) bounds for $\mu$ are
$$
0<\mu<\frac{2}{\lambda_\max}.
$$
In a practical environment we usually do not compute the eigenvalues of $\uu R{}_x$, and instead use simpler and **more conservative** policies. For example, as we know that the trace operator is invariant through base changes, and $\lambda_\max\leq\sum_k\lambda_k=\text{tr}\l(\uu R{}_x\r)$,
$$
\lambda_\max\leq\sum_{k=1}^N\lambda_k=\sum_{l=0}^{N-1}r_x[0]=N\cdot r_x[0]\implies\\
\implies\boxed{0<\mu<\frac{2}{N\cdot r_x[0]}.}
$$
The **speed of convergence** can be quantized as the number of iterations ($N_\text{iter}$) that are necessary to reduce the distance between the achieved solution and the optimum to a given value $\varepsilon$. In a given dimension, we can write:
$$
z_i^{k+1}=(1-\mu\lambda_i)^kz_i^0,\\\lvert1-\mu\lambda_i\rvert^{N_\text{iter}}=\varepsilon\iff\boxed{N_\text{iter}=\frac{\ln\varepsilon}{\ln{\lvert1-\mu\lambda_i\rvert}}.}
$$
When generalized to the $N$ dimensions, it can be shown that for small values of $\mu$,
$$
\boxed{N_\text{iter}\propto-\ln\varepsilon\frac{\lambda_\max}{\lambda_\min}.}
$$
so, the speed of convergence is **proportional to the dispersion of the eigenvalues**. (examples of this in slides 26-31 of section 3.3).

### Least Mean Square (LMS) approach

#### Stochastic approximation of the gradient

The steepest descent recursion for the Wiener-Hopf filter uses the correlation matrix and the cross-correlation vector, but **in a real setting, neither of those are known** and both have to be estimated via their **instantaneous approximations**. In this application, as signals are assumed to be non-stationary, we cannot use an estimator with a large memory (no accumulation of previous data). With these instantaneous estimations, we produce the **stochastic approximation of the gradient**:
$$
\l.\begin{matrix}
\uu R{}_x=\expected{\u x[n]\u x^T[n]}&\implies&\hat{\uu R}{}_x(\u x)=\u x[n]\u x^T[n]\\
\u r{}_{xd}=\expected{\u x[n]d[n]}&\implies&\hat{\u r}{}_{xd}(\u x,d)=\u x[n]d[n]
\end{matrix}\r\}\implies\\
\implies\nabla_\u h\expected{e^2[n]}\Big\vert_{\u h^k}=-2\u r{}_{xd}+2\uu R{}_x\u h^k\approx\boxed{-2\u x[n]d[n]+2\u x[n]\u x^T[n]\u h^{\color{red}{n}}.}
$$
Note that now $\u h^n$ is **estimated at each new sample** and the iterations and the samples are not different indexes anymore. Using this stochastic approximation, the recursion for the algorithm is:
$$
\u h^{k+1}=\u h^k+\mu\l(\u r{}_{xd}-\u{\u R}{}_x\u h^k\r)\implies\u h^{n+1}=\u h^n+\mu\Big(\u x[n]d[n]-\u x[n]\u x^T[n]\u h^n\Big).
$$
Using the definition of the error, $e[n]=d[n]-\u h^T\u x[n]$, and manipulating a bit the previous expression, we get
$$
\u h^{n+1}=\u h^n+\mu\u x[n]\Big(d[n]-\u x^T[n]\u h^n\Big),
$$
and, as $\u u^T\u v=\u v^T\u u$, this is the same as
$$
\boxed{\u h^{n+1}=\u h^n+\mu\u x[n]e[n].}
$$
This is called the **Least Mean Squares (LMS)** approach. The **LMS algorithm** is, therefore,

1. Filter the received signal: $y[n]=\u x^T[n]\u h^n.$
2. Compute the error: $e[n]=d[n]-y[n].$
3. Update the coefficients: $\u h^{n+1}=\u h^n+\mu\u x[n]e[n].$
4. Return to 1 (if any more observations are received).

#### Convergence analysis of the LMS algorithm

We study the convergence of the LMS algorithm in a **stationary scenario**. As the gradient is estimated, the resulting value is **random**. Therefore, we need to **study the algorithm convergence in statistical terms**, in an expected value sense:
$$
\u h^{n+1}=\u h^n+\mu\Big(\u x[n]d[n]-\u x[n]\u x^T[n]\u h^n\Big),\\
\expected{\u h^{n+1}}=\expected{\u h^n}+\mu\expected{\u x[n]d[n]}-\mu\expected{\u x[n]\u x^T[n]\u h^n}.
$$
If we assume that the observations and the filter coefficients are approximately independent, we obtain the following equation:
$$
\expected{\u h^{n+1}}=\expected{\u h^n}+\mu\expected{\u x[n]d[n]}-\mu\expected{\u x[n]\u x^T[n]}\expected{\u h^n},\\
\boxed{\expected{\u h^{n+1}}=\expected{\u h^n}+\mu\u r{}_{xd}-\mu\uu R{}_x\expected{\u h^n}.}
$$
In the expected value sense, we have obtained **the same iteration equation** as with the steepest descent method. The **step size** $\mu$ has to fulfill the same restrictions as in the Steepest Descent (SD) algorithm to achieve convergence:
$$
0<\mu<\frac{2}{\lambda_\max}
$$
The **speed of convergence** is the same in both cases (LMS, in the expected value sense, and SD):
$$
N_\text{iter}\propto-\ln\varepsilon\frac{\lambda_\max}{\lambda_\min}
$$
And, as in the SD algorithm, a **more conservative policy** is adopted for the step size:
$$
0<\mu<\frac{2}{N\hat r_x[0]}
$$
In some cases, the dynamics of the input signal (that is, $r_x[0]$) are not constant due to non-stationarity. In such a case, the step size should be updated to guarantee convergence. So, the **normalized LMS** approach is:

- Use a **conservative value** for the step size: $\mu=\frac{2\alpha}{N\hat r_x[0]}$, with $0<\alpha<1$.
- **Dynamically estimate** the input power:
  - With an instantaneous estimation: $N\hat r_x[0]=\u x^T[n]\u x[n]$.
  - With a time-averaged estimation: $\hat r_x[0;n]=\gamma\hat r_x[0;n-1]+(1-\gamma)\lvert x[n]\rvert^2$.

#### Misadjustment of the LMS algorithm

Although the LMS algorithm converges in the expected value sense, the fact of estimating the gradient produces an **increase in variance** of the minimum error achieved. This is known as the **LMS steady-state excess MSE** (in absolute value, first) or as the **LMS misadjustment** (in relative terms, second):
$$
\expected{\hat e^2[n]}-\varepsilon\approx\frac{\mu\varepsilon\sum_{i=1}^N\lambda_i}{2-\mu\sum_{i=1}^N\lambda_i}=\frac{\mu Nr_x[0]}{2-\mu Nr_x[0]}.\\
M=\frac{\expected{\hat e^2[n]}-\varepsilon}{\varepsilon}\approx\frac{\mu Nr_x[0]}{2-\mu Nr_x[0]}\implies\text{if }\mu\ll\frac{2}{Nr_x[0]}\implies\\
\implies\boxed{M\approx\frac{\mu}{2}\sum_{i=1}^N\lambda_i=\frac{\mu}{2}Nr_x[0].}
$$
The $\mu$ parameter in the LMS algorithm:

- Is bounded to ensure convergence.
- The speed of convergence increases with $\mu$.
- The misadjustment is proportional to $\mu$.

Moreover, eigenvalue dispersion affects the speed of convergence, but not the misadjustment. The latter is mainly affected by increases in the power of the signal. (examples of this in slides 39-48 of section 3.3).

## 3.4. Applications of Optimal Filtering

This applications and how to solve them are all in the course slides of section 3.4. There, we solve the following problems:

- **Affine predictor:** Comparison between linear and affine prediction for non-zero mean signals.
- **Wiener-Hopf solution for highly correlated data:** Avoiding the use of close to singular correlation matrices.
- **Short term / Long term correlation:** Embedding a signal into noise. We separate a broadband noise signal from a narrowband signal that we want.