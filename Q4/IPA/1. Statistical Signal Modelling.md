[TOC]

# 1. Statistical Signal Modelling

## 1.1. Introduction to IPA and random variables

## 1.2. Modelling of memoryless sources/processes

### 1.2.a. Sample-wise operators

#### 1. Introduction

##### Stochastic Processes

When we have a set of signals that can be analyzed as being the result of the same given experiment, we model these signals as a **stochastic process** to jointly study them.

##### Memoryless Processes

The analysis of a stochastic process relies on the **<span style='color:blue'>statistic</span>** of the samples and on the **<span style='color:blue'>dependencies</span>** among samples. **Memoryless processes** assume that every sample of the process is **<span style='color:blue'>independent</span>** of its neighbor samples.

Any **processing of memoryless processes** only takes into account the sample values (**<span style='color:blue'>sample-wise operators</span>**), but not their index (time instant or position) of their neighbor samples' values. This are non-linear operations, defined by a mapping function, they process in the same way all samples with the same value and are commonly used for **<span style='color:blue'>perceptual purposes</span>**.

##### Image model

Sample-wise operators are largely used in **image processing**; in the **<span style='color:blue'>pixel-based image model</span>**, for example, the image is understood as a collection of independent pixels. Operations also only take into account the pixel values, manipulating equally every pixel with the same value. **Pixel-based image operators** are defined as following:

- In a generic way, without taking into account the specificity of the images. These are called **range transform operators**.
- In a specific way, adapting the operator to the image pixels' statistics. These are called **histogram-based operators**.

##### Memoryless operators

These operators are very fast since they only require accessing at the pixel value of the pixel being processed. They are **memoryless** since they don't require storing any neighbor pixel values.

Other image models require analyzing a neighborhood of the pixel being processed: space-frequency data, geometrical data, region-based models... We will analyze the two main types of memoryless operators introduced before.

#### 2. Generic operators

##### Range Transform Operators

We define a transformation $T(\cdot)$ on the range of values of the input image ($r$) onto the range of values of the output image ($s$).

###### Gray level mapping

Different segments of the input gray-level range are **<span style='color:blue'>expanded or compressed</span>** depending on the transform features. The segments of $r$ where the magnitude of the derivative of $T(r)$ is greater than 1 are expanded, and the segments where it's smaller than 1 are compressed.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212223104997.png" alt="image-20200212223104997" style="zoom:80%;" />

###### Contrast mapping

It expands (stretches) a range of the input image, mapping it into the whole output image range. A set of values of $r$ are mapped into a single value of $s$, a process called **clipping**.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212223244793.png" alt="image-20200212223244793" style="zoom:80%;" />

Typically, clipping happens between the ends of the $r$ range and the ends of the $s$ range, mapping the minimum onto the minimum and the maximum onto the maximum. As a consequence of clipping, contrast mapping is a non-reversible transform (it's not injective).

###### Negative mapping

It inverts the range of values of the input image creating a negative version of it. This does not change the contrast of the image. The difference between two pixels remains the same, and the magnitude of the derivative of $T$ is always 1.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212223756207.png" alt="image-20200212223756207" style="zoom:80%;" />

###### Binarization mapping

It binarizes the image by clipping all values below a threshold to 0 and all values above to 1. This is also commonly known as thresholding, and is non-reversible because of clipping.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212223909778.png" alt="image-20200212223909778" style="zoom:80%;" />

###### Log transformation

It is mainly used to compress the dynamic range of the image.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212223952176.png" alt="image-20200212223952176" style="zoom:80%;" />

###### Power-law

Also known as *gamma correction*, this was originally developed for correcting Cathode Ray Tubes (CRT) distortion. It's useful to implement the Stevens power law for brightness perception.

###### Pseudo color

The input range is mapped onto a higher dimensional space; for example, a 3D space representing a color space, where a color ($s$) is assigned to every grey level value ($r$). The transform can be represented by means of a Look-Up Table (LUT) that puts in correspondence the input and output values.

<img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200212224404717.png" alt="image-20200212224404717" style="zoom: 80%;" />

It's commonly used for visualization purposes in arts and biomedical applications. In satellite imagery, for example, spectral indexes are combinations between bands to obtain a parameter of interest.

##### Implementation

Let us assume we want to implement $s=T(r)=\log(r+1)$. A possible approach is to compute the transform sample by sample:

```c++
// Transform the image
for (int i = 0; i < M; ++i) {
    for (int j = 0; j < N; ++j) y[i][j] = log(x[i][j] + 1);
}
```

But the resulting image has only 256 possible different values. So, we improve the implementation using dynamic programming:

```c++
// Build the LUT
for (int k = 0; k < Max_Val; ++k) LUT[k] = log(k + 1);

// Transform the image using the LUT
for (int i = 0; i < M; ++i) {
    for (int j = 0; j < N; ++j) y[i][j] = LUT(x[i][j]);
}
```

This replaces runtime computation with simple array indexing operations.

#### 3. Histogram-based operators

##### Histogram definition

The **histogram** $h(r_k)$ of a grey-level image with range $[0\ldots L-1]$ is a discrete map that stores for each possible pixel value ($r_k$) the **<span style='color:blue'>number of ocurrences</span>** of that value in the image ($n_k$); that is, the number of pixels in the image with a given grey-level value. The histogram information is **related to the probability of occurrence of a given value** in the image. The **normalized histogram** $p(r_k)$ is an estimation of the **<span style='color:blue'>probabilty density function</span>** (pdf) of a random variable associated to the grey-level values of the image pixels:
$$
p(r_k)=\frac{h(r_k)}{\sum_{k=0}^{L-1}h(r_k)}=\frac{n_k}{\sum_{k=0}^{L-1}n_k}=\frac{n_k}{n},\forall r_k\in[0\ldots L-1].
$$

###### Histogram computation

```c++
// Compute the histogram of an image
const IMG_MAX_VAL;
vector<int> h(IMG_MAX_VAL, 0);
for (auto u: img) {
    for (auto v: u) ++h[v];
}
```

###### Examples

<img src="1. Statistical Signal Modelling.assets/dark.png" style="zoom:40%;" /><img src="1. Statistical Signal Modelling.assets/bright.png" style="zoom:40%;" />

<img src="1. Statistical Signal Modelling.assets/low_contrast.png" style="zoom:40%;" /><img src="1. Statistical Signal Modelling.assets/high_contrast.png" style="zoom:40%;" />

###### Color image histogram

The histogram of a **color image** can be defined in several ways:

1. A separate histogram for each component
   - This does not represent the joint probability of the three color components.
2. A 3D histogram (joint histogram)
   - This counts the occurrences of every possible colour $(c_1,c_2,c_3)$.
   - A matrix of $L^3$ elements is created, typically $256^3$.
3. A luminance 1D histogram and a joint 2D chrominance histogram

##### Histogram equalization

###### Continuous case

**Histogram equalization** implements a pixel-based transform aiming at producing a **flat histogram output image**. It increases the global contrast and it depends on the input image's histogram. In this case, we can use the following change of variable result:

If $x$ is a continuous variable and $y=g(x)$ is a strictly monotonous map with inverse map $x=h(y)$, then the pdf of $y=g(x)$ is given by:
$$
f_y(y)=\left|\frac{dy}{dx}\right|^{-1}f_x(x)=\left|\frac{dg(x)}{dx}\right|^{-1}f_x(x)
$$

###### From continuous to discrete case

A mapping using the curve of the accumulated probability of $x$ produces an output image **with a uniform pdf** (equalized in the $[0,1]$ interval).
$$
f_y(y)=\left|\frac{dg(x)}{dx}\right|^{-1}f_x(x);\quad f_y(y)=1\implies\left|\frac{dg(x)}{dx}\right|=f_x(x)\implies y=g(x)=\int_{-\infty}^xf_x(w)dw
$$
The continuous case mapping has to be adapted to the discrete case:

- **Constraint:** elements having originally the same value (*being in the same bin*) should receive the same value (*be in the same bin*) after the transformation.
- **Note:** two different input bins can be merged into a single transformed one.

$$
s=T(r)=\int_0^rf_r(w)dw\Longleftrightarrow s_k=T(r_k)=\sum_{j=0}^kp(r_j)=\sum_{j=0}^k\frac{n_j}{n}.
$$

###### Discrete case

The resulting values ($s_k$) are defined on the range $[0,1]$. In order to have the values in the range $\{0\ldots L-1\}$, they should be scaled and rounded. One possible approach is
$$
s_k=T(r_k)=\sum_{j=0}^kp(r_j)=\sum_{j=0}^k\frac{n_j}{n};\qquad t_k=\text{round}((L-1)s_k).
$$
The final equalization maps all pixels with value $r_k$ into the value $t_k$. In this example, we can see how the **<span style='color:red'>histogram</span>** of an image gets equalized and how the **cumulative histogram** gets closer to the $y=x$ line, while observing the increasing effects on the contrast.

<img src="1. Statistical Signal Modelling.assets/histogram1.png" style="zoom:35%;" /><img src="1. Statistical Signal Modelling.assets/histogram1_equalized.png" style="zoom:35%;" />

#### 4. Conclusions

In the **pixel-based image model**, operations only take into account the values of the pixels (<span style='color:blue'>point-wise operators</span>), but neither their position nor the values of their neighbor pixels.

In range transform, a **mapping** $T(\cdot)$ is defined on the range of values of the input image $r$ onto the range of values of the output image $s$.

- The mapping **expands/contracts** segments of the input range depending on the magnitude of the derivative of the transform.
- If the mapping is not bijective, **it cannot be inverted**.

The **histogram information** is related to the probability of occurrence of a given value in the image.

- If the histogram of an image is known, **specific transforms** such as the equalization transform can be defined for that image.

### 1.2.b. Quantization

#### 1. Introduction

A very common and useful sample-wise operation is **quantization**. This operation is involved in nearly all digital signal processing.

**Scalar quantization** maps values from a scalar **continuous signal**, or a discrete one with very high resolution, onto a **finite set of values**. Truncation and rounding are simple cases of quantization. This can be also extended to **vector quantization**, a very useful tool. The difference between an input value and its quantized version is referred to as **quantization error**.

**<span style='color:blue'>Storage</span>** implies quantization. All kinds of data are to be stored: original data, transformed data, computed descriptors... Quantization carries a loss of quality: it's a **non-reversible operation**. It is necessary, then, to define **measures of quality** to assess the performance of the quantization. Commonly, **distortion measures** $d(x,q(x))$ are defined: MSE, SNR, Perceptual measures... The assessment will be done in statistical terms with the expected value of this distortion, $\mathbb E[d(X,q(X))]$.

##### Distortion/Quality measures

The error is $e[n]=x[n]-q[n]$. Different measures are defined here:

- **Mean Square Error** (MSE): $\sigma^2_{\text{MSE}}=\sigma_e^2=\mathbb E[|e[n]|^2]=\frac{1}{N}\sum_{i=1}^N|e[n]|^2.$
  - Estimation of expectation.
- **Mean Absolute Difference** (MAD): $C_{\text{MAD}}=\mathbb E[|e[n]|]=\frac{1}{N}\sum_{i=1}^N|e[n]|.$
  - Faster computation, but less sensitive to outliers.
- **Signal to Noise Ratio** (SNR): $SNR=10\log_{10}\frac{\sigma_x^2}{\sigma_e^2},\ (dB).$
  - Comparison of estimated signal powers.
- **Peak Signal to Noise Ratio** (PSNR): $PSNR=10\log_{10}\frac{M^2}{\sigma_e^2},\ (dB).$
  - Very used in signal processing. $M=$ maximum peak-to-peak value of the representation.

#### 2. Uniform quantization

##### Quantization

The input ($x$) is a **<span style='color: blue'>scalar analog value</span>** or a digital one with very high resolution. The output ($y$) is one from N possible values. A **<span style='color:blue'>quantizer</span>** $q(\cdot)$ performs a mapping $q:R\rightarrow C.$ The **encoder** can be seen as a <span style='color:brown'>selection of a cell in a partition</span>: it **classifies** $x$ as $i=\alpha(x)$, where the index links to a set of disjoint cells $S=\{S_i\}_{i\in\{1,2,\ldots,n\}}$ that forms a partition of the domain of $x$, $i\neq j\implies S_i\cap S_j=\emptyset$. Then, the **decoder** can be seen as a <span style='color:brown'>selection of a codeword</span> (cell representative): it maps an index $j$ to a representative of the cell $S_j$ as $y_j=\beta(j)$, where $C=\{y_1,y_2,\ldots,y_n\}\sub R$. There fore, the quantizer is defined as:
$$
q(x)=\beta(\alpha(x));\quad q\equiv\{S,C\}=\{\alpha,\beta\}.
$$
The quantization step introduces losses in the signal representation that have to be assessed. These losses are defined as the **quantization error**, $e_x=x-q(x).$

<span style='color:green'>❖</span> **Fixed length encoding:** the entropic coding of the source created by the quantized signal assuming fixed length codes leads to a code rate of $r=\lceil\log_2n\rceil=\lceil\log_2|C|\rceil$ bits/symbol.

##### Mid-rise quantization

Levels $y_i=\beta(\alpha(x))$ are equispaced with a space of $\Delta$. The thresholds $a_i$ are midway between levels.

<img src="1. Statistical Signal Modelling.assets/image-20200217191705775.png" alt="image-20200217191705775" />

The quantization could be expressed as $q_{\text{mr}}(x)=\Delta\cdot\left(\left\lfloor\frac{x}{\Delta}\right\rfloor+\frac{1}{2}\right)$, and the decomposition into encoder and decoder would be
$$
\alpha(x)=\left\lfloor\frac{x}{\Delta}\right\rfloor,\quad\beta(k)=\Delta\cdot\left(k+\frac{1}{2}\right).
$$


##### Mid-tread quantization

Levels $y_i=\beta(\alpha(x))$ are equispaced with a space of $\Delta$, the same as before, but now the values around zero have image zero:

![image-20200217192148604](1. Statistical Signal Modelling.assets/image-20200217192148604.png)

The quantization could be expressed as $q_{\text{mt}}(x)=\Delta\cdot\left\lfloor\frac{x}{\Delta}+\frac{1}{2}\right\rfloor$, and the decomposition into encoder and decoder would be
$$
\alpha(x)=\left\lfloor\frac{x}{\Delta}+\frac{1}{2}\right\rfloor,\quad\beta(k)=\Delta\cdot k.
$$
**Note:** using mid-tread quantization, **one quantization level is lost**, which is negligible when $L$ is large, but may be important for small values. Also, **mid-tread has a zero output level**, while mid-rise **does not**.

##### Quality measure: Distortion

The **distortion** is based on a measure $d(x,y)$ that quantifies the cost of substituting the actual value $x$ by its representative $y$; for example, the squared error $d(x,y)=(x-y)^2.$ It's actually computed as the expected value of $d(x,y)$ over the input $X$:
$$
D(q)=\mathbb E[d(X,q(X))]=\int_{\mathcal X}d(x,q(x))f_X(x)dx=\sum_{i=1}^LP(y_i)\mathbb E[d(X,y_i)|X\in S_i]
$$
**<span style='color: blue'>Example</span>:** a uniform quantizer with $L$ levels. $X$ presents a uniform pdf, and the distortion error is the squared error. Then,
$$
D(q)=\sum_{i=1}^L\int_{I_i}(x-y_i)^2f_X(x)dx=\left[f_X(x\in I_i)=\frac{1}{L\Delta}\right]=\frac{1}{L\Delta}\sum_{i=1}^L\int_{I_i}(x-y_i)^2dx=\\ \left[\text{all integrals are the same}\right]=\frac{1}{\Delta}\int_0^\Delta\left(x-\frac{\Delta}{2}\right)^2dx=\frac{1}{\Delta}\left[\left(x-\frac{\Delta}{2}\right)^3\right]_0^\Delta=\boxed{\frac{\Delta^2}{12}}.
$$

This result is a good approximation for smooth pdf's. We may note this is exactly the MSE of this quantizer.

**<span style='color: blue'>Example</span>:** Show that a signal that is quantized with a uniform, scalar quantizer increases its quality (measured with the **Signal to Noise Ratio**) in $6\ dB$ with every additional bit used in the quantizer. You may assume:

- The signal $x[n]$ is always within the interval $[-A_x,A_x];$
- A *mid-rise uniform quantizer* of $B$ bits is used;
- The signal $x[n]$ is uniformly distributed within the quantization step $\Delta$, and
- The signal power can be approximated by $\sigma_x^2\approx kA_x^2,$ where $k$ is a constant value that depends on the kind of signal.
- $\log_{10}2\approx0.3.$

$$
\text{SNR}(B)=10\log_{10}\frac{\sigma_x^2}{\sigma_e^2}=\left[\sigma^2_x\approx kA_x^2;\ \sigma_e^2=\frac{\Delta^2}{12}\right]=10\log_{10}\frac{12kA_x^2}{\Delta^2}=\\\left[\text{using the length of }x,\ 2A_x=2^B\Delta\right]=10\log_{10}\frac{3k2^{2B}\Delta^2}{\Delta^2}=\\10\log_{10}(3k)+20B\log_{10}2=\left[\log_{10}2\approx0.3\right]\approx k'+6B.\quad (dB)
$$

#### 3. Non-uniform quantization

##### Motivation

When using uniform quantizers, the quantization error is **independent** of the signal level; the SNR for low level samples is **smaller** than for high level samples. For some kinds of sources, this feature is **not desirable**. For example, in voice signal, low amplitude samples are perceptually important. A **logarithm compressor-expander** is used, called a **compander**. Its range varies through time, therefore it's called an **adaptive quantizer**. Its pdf is approximately **Laplacian** (non-uniform quantizer).

##### Compander

**<span style='color:brown'>Compander modeling of non-uniform quantizers</span>:** Large amplitudes are first compressed, then uniformly quantized, and in the end, expanded again.

<img src="1. Statistical Signal Modelling.assets/image-20200218070646648.png" alt="image-20200218070646648" style="zoom:80%;" />

Signals with small amplitude have a small quantization step, while signals with large amplitude have a large quantization step. As a result, the **SNR** adapts to the signal, becoming robust to the input signal level.

##### A-Law and $\mu$-Law

There exist different implementations:

###### Power-Law companding

- $c(|x|)=|x|^p,$ for $0<p<1$ and usually $p\approx0.75.$
- This is very useful in audio coding for the coefficients of the transformed-domain polynomial approximations to the signal transform.

###### Logarithm Law

- It keeps the ratio $\frac{y_i}{\Delta_i}$ constant.
- In Europe and America, different laws (called the A-Law and the $\mu$-Law, respectively) are used for speech samples in telephony.

#### 4. Optimal quantization

##### Motivation

**Color image motivation (vector quantization):** given an image and a maximum number of colors to represent it, a <span style='color:blue'>non-uniform quantization</span> can be obtained, leading to the **optimum palette**; the optimum palette represents the image minimizing a distortion measure.

##### Optimal quantization

Given a source $X$, characterized by its pdf, define a quantizer $q(x)$ with:

- A given number of levels $L$
- Leading to the minimum distortion $D(q,x)$.

There is no closed-form solution to this problem, but there are some theoretical results and some **interesting algorithms**. Stuart P. Lloyd proved the following necessary conditions:

- The encoder $(\alpha,S_i)$ must be optimal given the decoder $(\beta,y_i).$
- The decoder $(\beta,y_i)$ must be optimal given the encoder $(\alpha,S_i).$

These conditions suggest an **iterative algorithm**. As it is easy to find the optimal $\alpha$ given $\beta$, and reciprocally it is easy to find the optimal $\beta$ given $\alpha$, the iterative algorithm should, given an initial approximation to one of the two, converge to a (presumably local) minimum. At each step, $D(q_i)$ decreases, and as $D(q_i)>0\ \forall i$, the algorithm converges. This conditions are not sufficient, though, so the algorithm may get caught in local minima of $D(q)$. Some results exist for particular pdf's.

##### Max-Lloyd Algorithm

Decision levels $a_k$ and representation values $y_k$ can be chosen to minimize a given criterion; for instance, the MSE. Given a random variable with a known pdf $p_X(x)$, we look for the $a_k$ and $y_k$ that minimize the MSE for a given number of levels $L$:
$$
\mathcal E=\mathbb E\left[(x-q(x)^2\right]=\int_{a_1}^{a_L}(x-q(x))^2f_x(x)dx=\sum_{k=1}^{L-1}\int_{a_k}^{a_{k+1}}(x-y_k)^2f_X(x)dx.
$$

###### Optimization of the MSE map

Computing the derivatives of $\mathcal E$ with respect to the variables ($a_k$ and $y_k$) and setting them equal to zero, we obtain:
$$
\frac{\part\mathcal E}{\part y_j}=\frac{\part}{\part y_j}\left(\sum_{k=1}^{L-1}\int_{a_k}^{a_{k+1}}(x-y_k)^2f_X(x)dx\right)=\frac{\part}{\part y_j}\left(\int_{a_j}^{a_{j+1}}(x-y_j)^2f_X(x)dx\right)=\\\int_{a_j}^{a_{j+1}}\frac{\part}{\part y_j}\left((x-y_j)^2\right)f_X(x)dx=-2\int_{a_j}^{a_{j+1}}(x-y_j)f_X(x)dx=0\iff\\\int_{a_j}^{a_{j+1}}xf_X(x)dx=y_j\int_{a_j}^{a_{j+1}}f_X(x)dx\iff\boxed{y_j=\frac{\int_{a_j}^{a_{j+1}}xf_X(x)dx}{\int_{a_j}^{a_{j+1}}f_X(x)dx}.}
$$

$$
\frac{\part\mathcal E}{\part a_j}=\frac{\part}{\part a_j}\left(\int_{a_{j-1}}^{a_j}(x-y_{j-1})^2f_X(x)dx+\int_{a_j}^{a_{j+1}}(x-y_j)^2f_X(x)dx\right)=\\ [\text{Fundamental Theorem of Calculus}]=(a_j-y_{j-1})^2f_x(a_j)-(a_j-y_j)^2f_x(a_j)=0\iff\\(a_j-y_{j-1})^2=(a_j-y_j)^2\iff y_j^2-y_{j-1}^2=2a_j(y_j-y_{j-1})\iff \boxed{a_j=\frac{y_j+y_{j-1}}{2}.}
$$

If $y_j=y_{j-1}$, then the algorithm has collapsed.

##### <span style='color:green'>Max-Lloyd Initialization</span>

<span style='color:green'>❖</span> Since the Max-Lloyd algorithm does not ensure reaching the optimum, the initialization step is very important; the algorithm may get trapped in the local minima closer to the initial approximation.

<span style='color:green'>❖</span> Several strategies have been proposed:

- **Random selection:** $N$ elements from the initial training data.
- **Regular lattice selection:** Product of uniform scalar quantizers.
- **Product codes:** Product derived from optimal scalar quantizers.
- **Splitting (or LBG algorithm):** Sequential optimizations.
  - Start from a quantization with $K\in\{1,2\}$ representatives.
  - Split each representative and optimize to obtain $2K$.
  - Iterate until reaching $N$ elements.

The final result depends on the correct initialization of the algorithm.

#### 5. Conclusions

**Memoryless processes** assume that every sample of the process is **independent** of its neighbor samples. The **processing of memoryless processes** only takes into account the sample values, but neither their index nor their neighbor sample values.

**Quantization** is involved in almost all of the digital signal processing field. **Storage** implies quantization, and quantization implies loss of information and quality: it is a non-reversible operation. We have studied **uniform quantization** (no information about the source), **non-uniform quantization** (with no statistical information available) and **optimal quantization** (statistical optimization).

### 1.2.c. Example: Sample-wise video processing

#### 1. Introduction

Many security applications require to prompt an alarm to trigger a recorder or to allow a human operator to evaluate the situation. False positives would be accepted.

To detect the presence of a person or object, it is not necessary to get its shape or trajectory, so the method can rely on a **sample-based model** of the image.

#### 2. Still background estimation

#### 3. Variable background estimation: Single Gaussian

#### 4. <span style='color:green'>Variable background estimation: Multiple Gaussians</span>

#### 5. Conclusions