---
title: 'LAB 5: LDA/QDA/RDA, kNN, Naïve Bayes'
author: "Lluís A. Belanche, translated to R Markdown by Marta Arias"
date: "March 2020"
output:
  html_document:
    df_print: paged
  html_notebook: default
pdf_document: default
subtitle: 'Example 3: k-Nearest Neighbor analysis'
editor_options:
  chunk_output_type: inline
---


k-NN can be found in several places in R packages; one of them is in the `class` package:
```{r}
library(class)
library(MASS)
```

k-NN is peculiar in the sense that training error is always 0 (unless there are inconsistencies in the data). So, you **must make sure** you use it correctly in the sense that **train and test sets cannot intersect**.

Its usage is:

`knn (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)`

This function predicts 'test' data; for each observation in 'test', looks for the k-nearest neighbors in 'train' (using plain Euclidean distance).

The classification is decided by majority vote, with ties broken at random (but if there are ties for the k-th nearest vector, all candidates are included in the vote)

`cl` is the vector of class labels for the observations in 'train'. If data is large, and k is not small, I would recommend to use prob = TRUE to get some estimations of posterior probabilities.

The following piece of code 
```{r}
# this fn selects uniformly at random n gaussians (whose centers are given by input M) 
# and then samples from the selected gaussians one sample for each selected center
generate <- function (M, n=100, Sigma=diag(2)*sqrt(1/4)) 
{
  z <- sample(1:nrow(M), n, replace=TRUE)
  t(apply(M[z,], 1, function(mu) mvrnorm(1, mu, Sigma)))
}

# generate 10 means in two dimensions
M0 <- mvrnorm(10, c(1,0), diag(2))

# generate data out of M0 (class 0)
x0 <- generate(M0)

# repeat with M1 (class 1)
M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- generate(M1)

# Bind them together (by rows)
train <- rbind(x0, x1)
(N <- dim(train)[1])

# generate class labels in {0,1}
t <- c(rep(0,100), rep(1,100))
```


Now generate a huge test data using a grid in the correct range:

```{r}
grid.size <- 100
XLIM <- range(train[,1])
grid.x <- seq(XLIM[1], XLIM[2], len=grid.size)

YLIM <- range(train[,2])
grid.y <- seq(YLIM[1], YLIM[2], len=grid.size)

test <- expand.grid(grid.x,grid.y)
dim(test)
```


Let's visualize 1-NN (only 1 neighbor) in action

```{r}
nicecolors <- c('black','red')

visualize.1NN <- function ()
{
  par(mfrow=c(1,1))

  predicted <- knn(train, test, t, k=1)

  # These are the predictions
  plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
  points(test, col=nicecolors[as.numeric(predicted)], pch=".")
  contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size), 
          levels=c(1,2), add=TRUE, drawlabels=FALSE)

  # Add training points, for reference
  points(train, col=nicecolors[t+1], pch=16)
  title("1-NN classification")
}
  
visualize.1NN ()
```


In order to see the effect of a different number of neighbors, let's do something nice:

```{r}
par(mfrow=c(2,3))

for (myk in c(1,3,5,7,10,round(sqrt(N))))
{
  predicted <- knn(train, test, t, k=myk)
    
  plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
  points(test, col=nicecolors[as.numeric(predicted)], pch=".")
  contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size), 
          levels=c(1,2), add=TRUE, drawlabels=FALSE)
  
  # add training points, for reference
  points(train, col=nicecolors[t+1], pch=16)
  title(paste(myk,"-NN classification",sep=""))
}
```

     
Possibly you have to "Zoom" the plot to see it properly..

This maximum value (sqrt of training set size) for the number of nearest neighbors is a popular rule of thumb ... but I do not have an explanation for it
Folklore also says that it should be a prime number ...

Now we can illustrate the method on a real problem: we use the previous wine data for comparison

First setup a k-NN model with $k=3$ neighbours
Notice there is no "learning" ... the data is the model (just test!)

```{r}
data(wine, package="bootcluster") 
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols','Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280.OD315','Proline')
wine$Wine.type <- factor(wine$Wine.type, labels=c("Cultivar.1","Cultivar.2","Cultivar.3"))

wine.data <- subset (wine, select = -Wine.type)
```

The following is a very crude way of performing LOOCV for k-NN :-)

```{r}
knn.preds <- rep(NA, nrow(wine.data))

for (i in 1:nrow(wine.data))
{
  knn.preds[i] <- knn (wine.data[-i,], wine.data[i,], wine$Wine.type[-i], k = 3) 
}

(tab <- table(Truth=wine$Wine.type, Preds=knn.preds))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
```


As usual, rows are true targets, columns are predictions (may I suggest that you adhere to this convention too).

One could also use the function `knn1()` when $k=1$ (just one neighbour).

How do we optimize the hyper-parameter `k`? One way is by using the LOOCV. Actually, there is an implementation of LOOCV for k-NN:

```{r}
myknn.cv <- knn.cv (wine.data, wine$Wine.type, k = 3)
(tab <- table(Truth=wine$Wine.type, Preds=myknn.cv))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
```


Note that the results may not fully coincide by the random tie-breaking mechanism.

As usual, for reproducibility:
```{r}
set.seed (6046)
```

Now, let us tune $k$; we'll try all values from $k=1$ to $k=13$ since $\sqrt{178}\approx 13.34$. We define a function `loop.k` that loops over all values of k, computes the loocv-error for each $k$ and stores it in matrix which is then given as its result.

```{r}
N <- nrow(wine)
neighbours <- 1:sqrt(N)

loop.k <- function (mydata, mytargets, myneighbours)
{
  errors <- matrix (nrow=length(myneighbours), ncol=2)
  colnames(errors) <- c("k","LOOCV error")

  for (k in myneighbours)
  {
    myknn.cv <- knn.cv (mydata, mytargets, k = myneighbours[k])
  
    # fill in number of neighbours and LOOCV error
    errors[k, "k"] <- myneighbours[k]
  
    tab <- table(Truth=mytargets, Preds=myknn.cv)
    errors[k, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
  }
  errors
}
```


We run our function and plot the results as a function of $k$.

```{r}
par(mfrow=c(1,1))

plot(loop.k (wine.data, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
```


It seems that kNN does a pretty bad job here; 1-NN is the best choice but the model is terrible, compared to that of LDA/QDA

In fact, k-NN normally benefits from standardizing the variables, so let us try:

```{r}
plot(loop.k (scale(wine.data), wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
```


... which is very much the case indeed! The prediction error has improved greatly.

An alternative idea would be to use the previously computed LDs

```{r}
lda.model <- lda (Wine.type ~ ., data = wine)
loadings <- as.matrix(wine.data) %*% as.matrix(lda.model$scaling)
```


Let's repeat the loop over k

```{r}
set.seed (6046)
plot(loop.k (loadings, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
```


So we would keep 6 neighours!

Notice that the tested values for k need not be consecutive; in a large dataset, this would be very time-consuming; also we would not use LOOCV for the same reason, but rather 10-CV
