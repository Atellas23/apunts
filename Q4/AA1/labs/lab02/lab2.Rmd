---
title: 'LAB 2: Visualization (PCA/FDA)'
author: "Llu√≠s A. Belanche, translated to R Markdown by Marta Arias"
date: "February 2020"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r}
set.seed(222)
library(MASS)
```


Fisher's discriminant analysis (FDA) is a method that finds a linear combination of variables to separate two or more classes of objects; Principal Component Analysis (PCA) was developed by Pearson: this method finds a linear combination of features to project or separate two or more classes of objects. Both methods are linear in that they extract new features as a linear combination of the original variables; in a sense, they can be understood as projection methods.

If your goal is to perform dimensionality reduction for class discrimination (separation), you should use FDA instead of PCA; PCA is useful for signal representation (but not necessarily for discrimination), and vice versa.

Let's start ...

### Example 1: Comparison between PCA and FDA on 2D toy data

Let `Sigma` be a 2x2 positive-definite symmetric matrix specifying the covariance matrix of two variables

```{r}
N <- 200
(Sigma <- matrix(data=c(2,1.3,1.3,1),nrow=2,ncol=2,byrow=TRUE))
```


These are the eigenvalues (should be strictly positive):
```{r}
eigen (Sigma, only.values=TRUE)$values
```

Let's create class 1 ('red' class)
```{r}
mean.1 <- matrix(c(2,0),nrow=2,ncol=1)                         
X.red <- mvrnorm(N, mu=mean.1, Sigma=Sigma)
```


Let's create class 2 ('green' class)

```{r}
mean.2 <- -mean.1    # notice the "-"
X.green <- mvrnorm(N, mu=mean.2, Sigma=Sigma)
```

```{r}
par(pty="s")
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), 
     xlim=c(-6,6),ylim=c(-6,6),
     col=c(rep('red',N),rep('green',N)), main="Toy data", xlab="X1", ylab="X2")
```

Now we glue both classes one after the other and create a dataframe

```{r}
d <- data.frame(c(rep(1,N),rep(2,N)), c(X.red[,1], X.green[,1]), c(X.red[,2], X.green[,2]))
colnames(d) <- c("target", "X1", "X2")
d$target <- as.factor(d$target)
summary(d)
```

#### Let's perform FDA

Call to FDA (also known as LDA, because there is a strong relationship with it, see a later lecture)
```{r}
myLDA <- lda(d[c(2,3)],d[,1])
```

Now we show the best projection direction on the original space. This direction maximizes the separability of the classes. 

```{r}
par(pty="s")
LDAslope <- myLDA$scaling[2]/myLDA$scaling[1]   # we first need the slope
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), col=c(rep('red',N),rep('green',N)),
     xlim=c(-6,6),ylim=c(-6,6),
     main="Direction for projection using FDA", xlab="X1", ylab="X2")

abline(0,LDAslope,col='black',lwd=2)
```

We can also compute the projections of the two classes

```{r}
myLDA.proj <- d[,2] * myLDA$scaling[1] + d[,3] * myLDA$scaling[2]

plot(myLDA.proj, c(rep(0,N),rep(0,N)), col=c(rep('green',N),rep('red',N)),
     main='FDA projection as seen in 1D', xlab="Discriminant", ylab="")
```


To understand what is going on, do:

```{r}
myLDA
```

with the coefficients of the linear discriminant:

```{r}
myLDA$scaling
```



So we are projecting the data into the direction that maximizes (linear) separability:

$\text{projection}(X) = X_1\cdot\text{myLDA\$scaling}[1] + X_2\cdot\text{myLDA\$scaling}[2]$

#### Now we compute PCA:
  
```{r}
(myPCA <- prcomp(d[c(2,3)], scale=TRUE))
```

Now we need to project the data in the first principal component

```{r}
d1PCA <- myPCA$x[,1]
PCAslope1 <- myPCA$rotation[2,1]/myPCA$rotation[1,1]
```

And now we can perform the visualization:
```{r}
par(pty="s")
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), col=c(rep('red',N),rep('green',N)),
     xlim=c(-6,6),ylim=c(-6,6),
     main="Direction for projection using PCA", xlab="X1", ylab="X2")

abline(0,PCAslope1,col='black',lwd=2)
```
```{r}
myPCA.proj <- d[,2] * myPCA$rotation[1,1] + d[,3] * myPCA$rotation[2,1]

plot(myPCA.proj, c(rep(0,N),rep(0,N)), col=c(rep('green',N),rep('red',N)),
     main='PCA projection to first principal component as seen in 1D', xlab="Discriminant", ylab="")
```

We can see that the FDA projection maximizes separability (as given by Fisher's criterion) while the PCA projection maximices variability (given by the projected variance).


***

### Example 2: Visualizing crabs with FDA


Campbell studied rock crabs of the genus "Leptograpsus" in 1974. One
species, Leptograpsus variegatus, had been split into two new species,
previously grouped by colour (orange and blue). Preserved specimens
loose their colour, so it was hoped that morphological differences
would enable museum material to be classified.

Data is available on 50 specimens of each sex of each species (so 200 in total),
collected on sight at Fremantle, Western Australia. Each specimen has
measurements on: the width of the frontal lobe (FL), the rear width (RW),
the length along the carapace midline (CL), the maximum width (CW) of 
the carapace, and the body depth (BD) in mm, in addition to
colour (that is, species) and sex.

```{r}
# the crabs data is in the MASS package
data(crabs)
```

As usual, we first have a look at the data
```{r}
?crabs
summary(crabs)
head(crabs)
```

The goal is to separate the 200 crabs into four classes, given by the 2x2 configurations for sex (Male/Female) and species (Blue/Orange)

```{r}
Crabs.class <- factor(paste(crabs[,1],crabs[,2],sep=""))
```

Now 'BF' stands now for 'Blue Female', and so on
```{r}
table(Crabs.class)
```

Using the rest of the variables as predictors (except 'index', which is only an index)
```{r}
Crabs <- crabs[,4:8]
summary(Crabs)
```


Various preliminary plots (notice all 5 predictors are continuous)
```{r}
par(mfrow=c(1,1))
boxplot(Crabs)

hist(Crabs$FL,col='red',breaks=20,xlab="", main='Frontal Lobe Size (mm)')
hist(Crabs$RW,col='red',breaks=20,xlab="", main='Rear Width (mm)')
hist(Crabs$CL,col='red',breaks=20,xlab="", main='Carapace Length (mm)')
hist(Crabs$CW,col='red',breaks=20,xlab="", main='Carapace Width (mm)')
hist(Crabs$BD,col='red',breaks=20,xlab="", main='Body Depth (mm)')
```


Now let's visualize data using FDA
```{r}
(lda.model <- lda(x=Crabs, grouping=Crabs.class))
plot(lda.model)
```


As there are four classes (called 'groups' in LDA/FDA), we get three linear discriminants (LDs) for projection (always the number of classes minus 1)

We first compute the loadings (the 'loadings' are simply the projected data) -- this time we do it more generally, using matrix multiplication

```{r}
loadings <- as.matrix(Crabs) %*% as.matrix(lda.model$scaling)
```


We are performing dimensionality reduction 5D --> 3D, and plotting the projected data into the first two LDs (the 2 most important dimensions)

We do our own plotting method, with color and legend:

```{r}
colors.crabs <- c('blue', 'lightblue', 'orange', 'yellow')
crabs.plot <- function (myloadings)
{
  plot (myloadings[,1], myloadings[,2], type="n", xlab="LD1", ylab="LD2")
  text (myloadings[,1], myloadings[,2], labels=crabs$index, col=colors.crabs[unclass(Crabs.class)], cex=.55)
  legend('topright', c("B-M","B-F","O-M","O-F"), fill=colors.crabs, cex=.55)
}
crabs.plot (loadings)
```



The result is quite satisfactory, right? We can see that the 5 continuous predictors do indeed represent 4 different crabs. 

We can also see that crabs of the Blue "variety" are less different (regarding males and females) than those in the Orange variety

If you would like to keep this new representation for later use (maybe to build a classifier on it), simply do:

```{r}
Crabs.new <- data.frame (New.feature = loadings, Target = Crabs.class)
summary(Crabs.new)
```



Now let's analyze the numerical output of lda() in more detail:

```{r}
lda.model
```


1. "Prior probabilities of groups" is self-explanatory (these are estimated from the data, but can be overriden by the 'prior' parameter)

2. "Group means" is also self-explanatory (these are our mu's)

3. "Coefficients of linear discriminants" are the scaling factors we have been using to project data. These have been normalized so that the within-groups covariance matrix is spherical (a multiple of the identity). 
This means that the larger the coefficient of a predictor, the more important the predictor is for the discrimination

4. The "Proportion of trace" is the proportion of between-class variance that is explained by successive discriminants (LDs). For instance, in our case LD1 explains 68.6% of the total between-class variance. In this case, the first two LDs account for 98.56% of total between-class variance, fairly close to 100%


We can interpret our plot so that the horizontal axis (LD1) separates the groups mainly by using FL, CW and BD;
the vertical axis (LD2) separates the groups mainly by using RW and some CL, etc

In this case, the third dimension (LD3) adds but a little bit of discriminatory information. 

Let's visualize the crabs in 3D:

```{r}
library(rgl)

# 3D scatterplot (can be rotated and zoomed with the mouse)
plot3d(loadings[,1], loadings[,2], loadings[,3], "LD1", "LD2", "LD3",
       size = 4, col=colors.crabs[unclass(Crabs.class)], main="Crabs Data")

text3d(loadings[,1], loadings[,2], loadings[,3], color = "black", texts=rownames(Crabs.new), adj = c(0.85, 0.85), cex=0.6)
```

#### A possible transformation
         
As the measurements are lengths, it could be sensible to take logarithms

```{r}
(lda.logmodel <- lda (x=log(Crabs), grouping=Crabs.class))
```


The model looks a bit better, given that he first two LDs now account for 99.09% of total between-class variance, very good indeed, so a 3D plot does not add anything visual

As an example, the first (log) LD is given by:

$LD1 = -31.2*log(FL) - 9.5*log(RW) - 9.8*log(CL) + 66*log(CW) - 18*log(BD)$

Get the new loadings

```{r}
logloadings <- as.matrix(log(Crabs)) %*% as.matrix(lda.logmodel$scaling)
```


Plot the projected data in the first two LDs

```{r}
crabs.plot (logloadings)
```

The first coordinate clearly expresses the difference between species, and the second the difference between sex!
